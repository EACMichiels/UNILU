{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Variational Circuits & Quantum Neural Networks\n",
    "\n",
    "\n",
    "<span style=\"color: red; font-weight: bold;\">\n",
    "Please replace the \"?\" signs by real code !\n",
    "</span>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<span style=\"color: blue; font-weight: bold;\">\n",
    "In this notebook, we implement several variational quantum circuits for a data classification task, so-called variational quantum classifiers (VQCs). \n",
    "</span>\n",
    "\n",
    "At one point, it was common to refer to a subset of VQCs as quantum neural networks (QNNs) in analogy with classical neural networks. Indeed, there are cases where structures borrowed from classical neural networks, such as convolution layers, play an important role in VQCs. In such cases where the analogy is strong, QNNs may be a useful description. But parameterized quantum circuits need not follow the general structure of a neural network; for example, not all data need to be loaded in the first (input) layer; we can load some data in the first layer, apply some gates and then load additional data (a process called data \"reuploading\"). We should therefore think of QNNs as a subset of parameterized quantum circuits, and we should not be limited in our exploration of useful quantum circuits by the analogy to classical neural networks.\n",
    "\n",
    "<span style=\"color: blue; font-weight: bold;\">The dataset being addressed in this notebook consists of images containing horizontal and vertical stripes, and our goal is to label unseen images into one of the two categories depending on the orientation of their line. We will accomplish this with a VQC. As we go, we will address ways in which the calculation can be improved and scaled. \n",
    "</span>\n",
    "\n",
    "The dataset here is exceptionally easy to classify classically. It has been chosen for its simplicity so we can focus on the quantum part of this problem, and look at how a dataset attribute might translate to a part of a quantum circuit. It is not reasonable to expect a quantum speed-up for such simple cases where classical algorithms are so efficient.\n",
    "\n",
    "By the end of this notebook you should be able to:\n",
    "\n",
    "*   Load data from an image into a quantum circuit\n",
    "*   Construct an ansatz for a VQC (or QNN), and adjust it to fit your problem\n",
    "*   Train your VQC/QNN and use it to make accurate predictions on test data\n",
    "*   Scale the problem, and recognize limits of current quantum computers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research Paper: \n",
    "[https://arxiv.org/abs/2405.00781](https://arxiv.org/abs/2405.00781)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Level Phases\n",
    "\n",
    "<span style=\"color: blue; font-weight: bold;\">\n",
    "\n",
    "- Image Classification, 16 pixels --> low accuracy score of 60%\n",
    "\n",
    "- Enhancement: more CNOT gates for horizontal pixels --> high accuracy score of 100%\n",
    "\n",
    "- Extension to 36 pixels with fewer iterations\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install qiskit[visualization]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install qiskit-machine-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install qiskit_aer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "\n",
    "We will start by constructing the data. Data sets are often not explicitly generated as part of the Qiskit patterns framework. But data type and preparation is critical to successfully applying quantum computing to machine learning. The code below defines a data set of images with set pixel dimensions. <span style=\"color: blue; font-weight: bold;\">One full row or column of the image is assigned the value $\\pi/2$, and the remaining pixels are assigned random values on the interval $(0,\\pi/4)$. The random values are noise in our data. </span>Glance through the code to make sure you understand how the images are generated. Later on we will scale up the images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code defines the images to be classified:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Total number of \"pixels\"/qubits\n",
    "size = 8\n",
    "# One dimension of the image (called vertical, but it doesn't matter). Must be a divisor of `size`\n",
    "vert_size = 2\n",
    "# The length of the line to be detected (yellow). Must be less than or equal to the smallest dimension of the image (`<=min(vert_size,size/vert_size)`\n",
    "line_size = 2\n",
    "\n",
    "\n",
    "def generate_dataset(num_images):\n",
    "    images = []\n",
    "    labels = []\n",
    "    hor_array = np.zeros((size - (line_size - 1) * vert_size, size))\n",
    "    ver_array = np.zeros((round(size / vert_size) * (vert_size - line_size + 1), size))\n",
    "\n",
    "    j = 0\n",
    "    for i in range(0, size - 1):\n",
    "        if i % (size / vert_size) <= (size / vert_size) - line_size:\n",
    "            for p in range(0, line_size):\n",
    "                hor_array[j][i + p] = ? / 2\n",
    "            j += 1\n",
    "\n",
    "    # Make two adjacent entries pi/2, then move down to the next row. Careful to avoid the \"pixels\" at size/vert_size - linesize, because we want to fold this list into a grid.\n",
    "\n",
    "    j = 0\n",
    "    for i in range(0, round(size / vert_size) * (vert_size - line_size + 1)):\n",
    "        for p in range(0, line_size):\n",
    "            ver_array[j][i + p * round(size / vert_size)] = ? / 2\n",
    "        j += 1\n",
    "\n",
    "    # Make entries pi/2, spaced by the length/rows, so that when folded, the entries appear on top of each other.\n",
    "\n",
    "    for n in range(num_images):\n",
    "        rng = np.random.randint(0, 2)\n",
    "        if rng == 0:\n",
    "            labels.append(-1)\n",
    "            random_image = np.random.randint(0, len(hor_array))\n",
    "            images.append(np.array(hor_array[random_image]))\n",
    "\n",
    "        elif rng == 1:\n",
    "            labels.append(1)\n",
    "            random_image = np.random.randint(0, len(ver_array))\n",
    "            images.append(np.array(ver_array[random_image]))\n",
    "            # Randomly select 0 or 1 for a horizontal or vertical array, assign the corresponding label.\n",
    "\n",
    "        # Create noise\n",
    "        for i in range(size):\n",
    "            if images[-1][i] == 0:\n",
    "                images[-1][i] = np.random.rand() * ? / 4\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "hor_size = round(size / vert_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue; font-weight: bold;\">\n",
    "Note that the code above has also generated labels indicated whether the images contain a vertical (+1) or horizontal (-1) line.</span> We will now use sklearn to split a data set of 100 images into a training and testing set (along with their corresponding labels). Here, we use $70%$ of the data set for training, with the remaining $30%$ withheld for testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "images, labels = generate_dataset(200)\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = ?(\n",
    "    ?, ?, test_size=???, random_state=246\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue; font-weight: bold;\">\n",
    "Let's plot a few elements of our data set to see what these lines look like:\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make subplot titles so we can identify categories\n",
    "titles = []\n",
    "for i in range(8):\n",
    "    title = \"category: \" + str(train_labels[i])\n",
    "    titles.append(title)\n",
    "\n",
    "# Generate a figure with nested images using subplots.\n",
    "fig, ax = plt.subplots(4, 2, figsize=(10, 6), subplot_kw={\"xticks\": [], \"yticks\": []})\n",
    "\n",
    "for i in range(8):\n",
    "    ax[i // 2, i % 2].imshow(\n",
    "        train_images[i].reshape(vert_size, hor_size),\n",
    "        aspect=\"equal\",\n",
    "    )\n",
    "    ax[i // 2, i % 2].set_title(titles[i])\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these images is still paired with its label in `train_labels` in a simple list form:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_labels[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational quantum classifier: a first attempt\n",
    "\n",
    "### Qiskit patterns step 1: Map the problem to a quantum circuit\n",
    "\n",
    "The goal is to find a function $f$ with parameters $\\theta$ that maps a data vector / image $\\vec{x}$ to the correct category: $f_\\theta(\\vec{x}) \\rightarrow \\pm1$. This will be accomplished using a VQC with few layers that can be identified by their distinct purposes:\n",
    "\n",
    "$$\n",
    "f_\\theta(\\vec{x}) = \\langle 0|U^{\\dagger}(\\vec{x})W^\\dagger(\\theta)OW(\\theta)U(\\vec{x})|0\\rangle\n",
    "$$\n",
    "\n",
    "<span style=\"color: blue; font-weight: bold;\">\n",
    "Here, $U(\\vec{x})$ is the encoding circuit, for which we have many options as seen in the course. $W(\\theta)$ is a variational, or trainable circuit block, and $\\theta$ is the set of parameters to be trained. Those parameters will be varied by classical optimization algorithms to find the set of parameters that yields the best classification of images by the quantum circuit. This variational circuit is sometimes called the \"ansatz\". Finally, $O$ is some observable that will be estimated using the Estimator primitive. </span>There is no constraint that forces the layers to come in this order, or even to be fully separate. One could have multiple variational and/or encoding layers in any order that is technically motivated.\n",
    "\n",
    "<span style=\"color: blue; font-weight: bold;\">\n",
    "We start by choosing a feature map to encode our data. We will use the `z_feature_map`, as it keeps circuit depths low compared to some other feature mappings.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.circuit.library import z_feature_map\n",
    "\n",
    "# One qubit per data feature\n",
    "num_qubits = len(train_images[0])\n",
    "\n",
    "# Data encoding\n",
    "# Note that qiskit orders parameters alphabetically. We assign the parameter prefix \"a\" to ensure our data encoding goes to the first part of the circuit, the feature mapping.\n",
    "feature_map = ?(?, parameter_prefix=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?.draw('mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?.decompose().draw(\"mpl\", style=\"clifford\", fold=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must now decide on an ansatz to be trained. There are many considerations when selecting an ansatz. A complete description is beyond the scope of this introduction; here we simply point out a few categories of considerations.\n",
    "\n",
    "1.  **Hardware:** All modern quantum computers are more prone to errors and more susceptible to noise than their classical counterparts. Using an ansatz that is excessively deep (especially in transpiled, two-qubit depth) will not produce good results. A related issue is that quantum computers have some qubit layout, meaning that some physical qubits are adjacent on the quantum computer, and others may be very far from each other. Entangling adjacent qubits does not increase the depth by too much, but entangling very distant qubits can increase depth substantially, as we must insert swap gates to move information onto qubits that are adjacent in order for them to be entangled.\n",
    "2.  **The problem:** Whenever you have some information about your problem that could guide your ansatz, make use of it. For example, the data in this lesson is made up of images of horizontal and vertical lines. One could consider what correlation between adjacent colors/values identifies an image of a horizontal or vertical line. What attributes of an ansatz would correspond to this correlation between adjacent pixels? We will revisit this point more technically later in this lesson. But for now, let us simply say that including entanglement and CNOT gates between qubits corresponding to adjacent pixels seems like a good idea. In the bigger picture, consider whether the problem is actually best solved using a quantum circuit, or whether classical algorithms might exist that can do as good a job.\n",
    "3.  **Number of parameters:** Each independently parameterized quantum gate in the circuit increases the space to be classically optimized, and this results in slower convergence. But as problems scale up, one may encounter *barren plateaus*. This term refers to a phenomenon where the optimization landscape of a variational quantum algorithm becomes exponentially flat and featureless as the problem size increases. This causes vanishing gradients, making it difficult to effectively train the algorithm[\\[1\\]](#references). Barren plateaus are relevant to variational quantum algorithms like VQCs/QNNs. It should be noted that the increasing number of parameters is not the only consideration in avoiding barren plateaus; other considerations include global cost functions and random parameter initialization.\n",
    "\n",
    "\n",
    "In this notebook we will see a few simple examples of good practices in ansatz construction. \n",
    "\n",
    "<span style=\"color: blue; font-weight: bold;\">\n",
    "Let us first try the ansatz below. We will return to revise it, later.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "\n",
    "# Initialize the circuit using the same number of qubits as the image has pixels\n",
    "qnn_circuit = QuantumCircuit(size)\n",
    "\n",
    "# We choose to have two variational parameters for each qubit.\n",
    "params = ParameterVector(\"θ\", length=2 * size)\n",
    "\n",
    "# A first variational layer for all qubits:\n",
    "for i in range(?):\n",
    "    qnn_circuit.ry(params[i], i)\n",
    "\n",
    "# Here is a list of qubit pairs between which we want CNOT gates. The choice of these is not yet obvious.\n",
    "qnn_cnot_list = [[0, 1], [1, 2], [2, 3]]\n",
    "\n",
    "for i in range(len(?)):\n",
    "    qnn_circuit.?(qnn_cnot_list[i][0], qnn_cnot_list[i][1])\n",
    "\n",
    "# The second variational layer for all qubits:\n",
    "for i in range(?):\n",
    "    qnn_circuit.rx(params[size + i], i)\n",
    "\n",
    "# Check the circuit depth, and the two-qubit gate depth\n",
    "print(qnn_circuit.decompose().depth())\n",
    "print(\n",
    "    f\"2+ qubit depth: {qnn_circuit.decompose().depth(lambda instr: len(instr.qubits) > 1)}\"\n",
    ")\n",
    "\n",
    "# Draw the circuit\n",
    "?.draw(\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data encoding and variational circuit prepared, we can combine them to form our full ansatz. <span style=\"color: blue; font-weight: bold;\">\n",
    "In this case, the components of our quantum circuit are quite analogous to those in neural networks, with $U(\\vec{x})$ being most similar to the layer that loads input values from the image, and $W(\\theta)$ being like the layer of variable \"weights\". </span>Since this analogy holds in this case, we are adopting \"qnn\" in some of our naming conventions; but this analogy should not be limiting in your exploration of VQCs.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QNN ansatz\n",
    "ansatz = qnn_circuit\n",
    "\n",
    "# Combine the feature map with the ansatz\n",
    "full_circuit = QuantumCircuit(num_qubits)\n",
    "full_circuit.compose(?, range(num_qubits), inplace=True)\n",
    "full_circuit.compose(?, range(num_qubits), inplace=True)\n",
    "\n",
    "# Display the circuit\n",
    "?.decompose().?(\"mpl\", style=\"clifford\", fold=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue; font-weight: bold;\">\n",
    "We must now define an observable, so we can use it in our cost function. We will obtain an expectation value for this observable using Estimator. If we have selected a good, problem-motivated ansatz, then each qubit will contain information relevant to classification.</span> One can add layers to combine information onto fewer qubits (called a *convolutional layer*), such that measurements are only needed on a subset of the qubits in the circuit (as in convolutional neural networks). Or one can measure some attribute from each qubit. <span style=\"color: blue; font-weight: bold;\">Here we will opt for the latter, so we include a `Z` operator for each qubit.</span> There is nothing unique about choosing $Z$, but it is well motivated:\n",
    "\n",
    "*   This is a binary classification task, and a measurement of $Z$ can yield two possible outcomes.\n",
    "*   The eigenvalues of $Z$ ($\\pm 1$) are reasonably well separated, and result in an estimator outcome in interval \\[-1, +1], where 0 can simply be used as a cutoff value.\n",
    "*   It is straightforward to measure in Pauli Z basis with no extra gate overhead.\n",
    "\n",
    "So, Z is a very natural choice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.quantum_info import SparsePauliOp\n",
    "\n",
    "observable = SparsePauliOp.from_list([(\"?\" * (num_qubits), 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparsePauliOp in Qiskit is a compact representation of an operator written as a sum of Pauli strings, optimized for speed and memory efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our quantum circuit and the observable we want to estimate. Now we need a few things in order to run and optimize this circuit. \n",
    "<span style=\"color: blue; font-weight: bold;\">First, we need a function to run a forward pass. Note that the function below takes in the `input_params` and `weight_params` separately. The former is the set of static parameters describing the data in an image, and the latter is the set of variable parameters to be optimized.</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.primitives import BaseEstimatorV2\n",
    "from qiskit.quantum_info.operators.base_operator import BaseOperator\n",
    "\n",
    "\n",
    "def forward(\n",
    "    circuit: QuantumCircuit,\n",
    "    input_params: np.ndarray,\n",
    "    weight_params: np.ndarray,\n",
    "    estimator: BaseEstimatorV2,\n",
    "    observable: BaseOperator,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Forward pass of the neural network.\n",
    "\n",
    "    Args:\n",
    "        circuit: circuit consisting of data loader gates and the neural network ansatz.\n",
    "        input_params: data encoding parameters.\n",
    "        weight_params: neural network ansatz parameters.\n",
    "        estimator: EstimatorV2 primitive.\n",
    "        observable: a single observable to compute the expectation over.\n",
    "\n",
    "    Returns:\n",
    "        expectation_values: an array (for one observable) or a matrix (for a sequence of observables) of expectation values.\n",
    "        Rows correspond to observables and columns to data samples.\n",
    "    \"\"\"\n",
    "    num_samples = input_params.shape[0]\n",
    "    weights = np.broadcast_to(weight_params, (num_samples, len(weight_params)))\n",
    "    params = np.concatenate((input_params, weights), axis=1)\n",
    "    pub = (circuit, observable, params)\n",
    "    job = estimator.run([pub])\n",
    "    result = job.result()[0]\n",
    "    expectation_values = result.data.evs\n",
    "\n",
    "    return expectation_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Qiskit, a Pub (Primitive Unified Bloc) is a fundamental unit of work that is used to define vectorized workloads for quantum circuits. PUBs aggregate elements from multiple arrays (observables and parameter values) and are essential for executing workloads in Qiskit's Sampler and Estimator primitives. Each PUB is represented as a tuple, where the first element is the circuit being executed, and subsequent elements provide the observables and parameter values needed for the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "<span style=\"color: blue; font-weight: bold;\">Next, we need a loss function to calculate the difference between the predicted and calculated values of the labels. The function will take in the labels predicted by the algorithm and the correct labels and return the mean squared difference. There any many different loss functions. Here, MSE is an example that we chose.</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(predict: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Mean squared error (MSE).\n",
    "\n",
    "    prediction: predictions from the forward pass of neural network.\n",
    "    target: true labels.\n",
    "\n",
    "    output: MSE loss.\n",
    "    \"\"\"\n",
    "    if len(predict.shape) <= 1:\n",
    "        return ((predict - target) ** 2).mean()\n",
    "    else:\n",
    "        raise AssertionError(\"input should be 1d-array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also define a slightly different loss function that is a <span style=\"color: blue; font-weight: bold;\">function of the variable parameters (weights), for use by the classical optimizer. This function only takes the ansatz parameters as input;</span> other variables for the forward pass and the loss are set as global parameters. <span style=\"color: blue; font-weight: bold;\">\n",
    "The optimizer will train the model by sampling different weights and attempting to lower the output of the cost/loss function.</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_weights(weight_params: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cost function for the optimizer to update the ansatz parameters.\n",
    "\n",
    "    weight_params: ansatz parameters to be updated by the optimizer.\n",
    "\n",
    "    output: MSE loss.\n",
    "    \"\"\"\n",
    "    predictions = forward(\n",
    "        circuit=circuit,\n",
    "        input_params=input_params,\n",
    "        weight_params=weight_params,\n",
    "        estimator=estimator,\n",
    "        observable=observable,\n",
    "    )\n",
    "\n",
    "    cost = ?(?=predictions, ?=target)\n",
    "    objective_func_vals.append(cost)\n",
    "\n",
    "    global iter\n",
    "    if iter % 50 == 0:\n",
    "        print(f\"Iter: {iter}, loss: {cost}\")\n",
    "    iter += 1\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we referred to using a classical optimizer. When we get to searching through weights to <span style=\"color: blue; font-weight: bold;\">\n",
    "minimize the cost function</span>, we will use the optimizer COBYLA:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set some initial global variables for the cost function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "circuit = full_circuit\n",
    "observables = observable\n",
    "# input_params = train_images_batch\n",
    "# target = train_labels_batch\n",
    "objective_func_vals = []\n",
    "iter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qiskit Patterns Step 2: Optimize problem for quantum execution\n",
    "\n",
    "We start by selecting a backend for execution. In this case, we will use the least-busy backend.\n",
    "**We use a Simulator in this notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.primitives import StatevectorSampler\n",
    "sampler = StatevectorSampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qiskit Patterns Step 3: Execute using Qiskit Primitives\n",
    "\n",
    "### Loop over the dataset in batches and epochs\n",
    "\n",
    "We first implement the full algorithm using a simulator for cursory debugging and for estimates of error. We can now go over the entire dataset in batches in desired number of epochs to train our quantum neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An epoch in machine learning is one complete pass of the training dataset through the model.\n",
    "\n",
    "If your training set has 10,000 samples, then 1 epoch means the model has seen all 10,000 once.\n",
    "- Epoch = Full pass over the entire dataset\n",
    "- Training usually needs multiple epochs because one pass is not enough for convergence.\n",
    "- During each epoch: the model computes predictions, the loss is calculated, gradients are computed, parameters are updated.\n",
    "\n",
    "Batch: the subset of samples processed before one update. \n",
    "\n",
    "Iteration: one update step.\n",
    "- If batch size = 100 and dataset = 10,000 samples → 100 iterations per epoch.\n",
    "- Iteration: one batch processed → one parameter update\n",
    "- Iterations per epoch = dataset size ÷ batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue; font-weight: bold;\">\n",
    "Here we take 1 epoch, a batch size of 140 and a dataset seize of 140.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.primitives import StatevectorEstimator as Estimator\n",
    "\n",
    "batch_size = 140\n",
    "num_epochs = ?\n",
    "num_samples = len(train_images)\n",
    "\n",
    "# Globals\n",
    "circuit = full_circuit\n",
    "estimator = Estimator()  # simulator for debugging\n",
    "observables = observable\n",
    "objective_func_vals = []\n",
    "iter = 0\n",
    "\n",
    "# Random initial weights for the ansatz\n",
    "np.random.seed(42)\n",
    "weight_params = np.random.rand(len(ansatz.parameters)) * 2 * np.pi\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range((num_samples - 1) // batch_size + 1):\n",
    "        print(f\"Epoch: {epoch}, batch: {i}\")\n",
    "        start_i = i * batch_size\n",
    "        end_i = start_i + batch_size\n",
    "        train_images_batch = np.array(train_images[start_i:end_i])\n",
    "        train_labels_batch = np.array(train_labels[start_i:end_i])\n",
    "        input_params = train_images_batch\n",
    "        target = train_labels_batch\n",
    "        iter = 0\n",
    "        res = minimize(\n",
    "            mse_loss_weights, weight_params, method=\"COBYLA\", options={\"maxiter\": 100}\n",
    "        )\n",
    "        weight_params = res[\"x\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qiskit Patterns Step 4: Post-process, return result in classical format\n",
    "\n",
    "### Testing and accuracy\n",
    "\n",
    "We now interpret the results from training. \n",
    "\n",
    "<span style=\"color: blue; font-weight: bold;\">\n",
    "We first test the training accuracy over the training set.\n",
    "</span> text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from qiskit.primitives import StatevectorEstimator as Estimator  # simulator\n",
    "# from qiskit_ibm_runtime import EstimatorV2 as Estimator  # real quantum computer\n",
    "\n",
    "estimator = Estimator()\n",
    "# estimator = Estimator(backend=backend)\n",
    "\n",
    "pred_train = forward(circuit, np.array(train_images), res[\"x\"], estimator, observable)\n",
    "# pred_train = forward(circuit_ibm, np.array(train_images), res['x'], estimator, observable_ibm)\n",
    "\n",
    "print(pred_train)\n",
    "\n",
    "pred_train_labels = copy.deepcopy(pred_train)\n",
    "pred_train_labels[pred_train_labels >= 0] = 1\n",
    "pred_train_labels[pred_train_labels < 0] = -1\n",
    "print(pred_train_labels)\n",
    "print(train_labels)\n",
    "\n",
    "accuracy = accuracy_score(?, ?)\n",
    "print(f\"Train accuracy: {? * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pred_train_labels))\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue; font-weight: bold;\">\n",
    "The training accuracy is only $60%$, which is definitely not good. It is hard to imagine that the model's performance on the test set could be any better. Let's verify.\n",
    "</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = forward(circuit, np.array(test_images), res[\"x\"], estimator, observable)\n",
    "# pred_test = forward(circuit_ibm, np.array(test_images), res['x'], estimator, observable_ibm)\n",
    "\n",
    "print(pred_test)\n",
    "\n",
    "pred_test_labels = copy.deepcopy(pred_test)\n",
    "pred_test_labels[pred_test_labels >= 0] = 1\n",
    "pred_test_labels[pred_test_labels < 0] = -1\n",
    "print(pred_test_labels)\n",
    "print(test_labels)\n",
    "\n",
    "accuracy = accuracy_score(?, ?)\n",
    "print(f\"Test accuracy: {? * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pred_test_labels))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is not classifying these data well. We should ask why this is, and in particular, we should check:\n",
    "\n",
    "<span style=\"color: blue; font-weight: bold;\">\n",
    "           \n",
    "- Did we stop the training too soon? Were more optimization steps needed?\n",
    "\n",
    "- Did we construct a bad Ansatz? This could mean a lot of things. When we work on real quantum computers, circuit depth will be a major consideration. The number of parameters is also potentially important, as is the entangling between qubits.\n",
    "\n",
    "- Combining the two above, did we construct an ansatz with too many parameters to be trainable?\n",
    "\n",
    "\n",
    "We can start by checking for convergence in the optimization:\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_func_vals_first = objective_func_vals\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(obj_func_vals_first, label=\"first ansatz\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might try extending the optimization steps to make sure the optimizer didn't just get stuck in a local minimum in parameter space. <span style=\"color: blue; font-weight: bold;\">But it looks fairly converged.</span> \n",
    "\n",
    "Let's take a closer look at the images that were *not* classified correctly, and see if we can understand what is happening.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed = []\n",
    "for i in range(len(test_labels)):\n",
    "    if pred_test_labels[i] != ?[i]:\n",
    "        missed.append(test_images[i])\n",
    "print(len(missed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(12, 2, figsize=(6, 6), subplot_kw={\"xticks\": [], \"yticks\": []})\n",
    "for i in range(len(missed)):\n",
    "    ax[i // 2, i % 2].imshow(\n",
    "        missed[i].reshape(vert_size, hor_size),\n",
    "        aspect=\"equal\",\n",
    "    )\n",
    "plt.subplots_adjust(wspace=0.02, hspace=0.025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue; font-weight: bold;\">We see 24 images.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that <span style=\"color: blue; font-weight: bold;\">the vast majority of the wrongly-classified images have a vertical line. Something about our model is failing to capture information about those.</span>You may have seen this coming, based on the first variational circuit. Let's look at it more closely.\n",
    "\n",
    "## Improving the model\n",
    "\n",
    "### Step 1 revisited\n",
    "\n",
    "In mapping our problem to a quantum circuit, we should have explicitly thought about the how the information in adjacent pixels determines class. In order to identify horizontal lines, we want to know \"if pixel $i$ is yellow, is pixel $i+1$ yellow\" for all the pixels across each row. We also want to know about vertical lines. But since the classification is binary, one could imagine simply saying that if such a horizontal line is *not* detected, then it is a vertical line. Our previous variational circuit contained CNOT gates between qubits (and therefore pixels) 0 & 1, 1 & 2, and 2 & 3. \n",
    "\n",
    "<span style=\"color: blue; font-weight: bold;\">That covers any horizontal lines across the top of the image, but it does not directly detect vertical lines, nor does it completely detect horizontal lines, as it ignores the lower row.</span>\n",
    "\n",
    "\n",
    "<span style=\"color: blue; font-weight: bold;\">To fully detect all horizontal lines, we would want to have a similar set of CNOT gates between qubits (pixels) 4 & 5, 5 & 6, and 6 & 7. We could keep in mind that adding CNOT gates between qubits corresponding to vertical lines (like 0 & 4, or 2 & 6) may also be useful. But we will first check whether it is sufficient to detect that there *is* or *is not* a horizontal line.</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the circuit using the same number of qubits as the image has pixels\n",
    "qnn_circuit = QuantumCircuit(size)\n",
    "\n",
    "# We choose to have two variational parameters for each qubit.\n",
    "params = ParameterVector(\"θ\", length=2 * size)\n",
    "\n",
    "# A first variational layer:\n",
    "for i in range(size):\n",
    "    qnn_circuit.ry(params[i], i)\n",
    "\n",
    "# Here is an extended list of qubit pairs between which we want CNOT gates. This now covers all pixels connected by horizontal lines.\n",
    "qnn_cnot_list = [[0, 1], [1, 2], [2, 3], [4, 5], [5, 6], [6, 7]]\n",
    "\n",
    "for i in range(len(qnn_cnot_list)):\n",
    "    qnn_circuit.cx(qnn_cnot_list[i][0], qnn_cnot_list[i][1])\n",
    "\n",
    "# The second variational layer:\n",
    "for i in range(size):\n",
    "    qnn_circuit.rx(params[size + i], i)\n",
    "\n",
    "# Check the circuit depth, and the two-qubit gate depth\n",
    "print(qnn_circuit.decompose().depth())\n",
    "print(\n",
    "    f\"2+ qubit depth: {qnn_circuit.decompose().depth(lambda instr: len(instr.qubits) > 1)}\"\n",
    ")\n",
    "\n",
    "# Combine the feature map and variational circuit\n",
    "ansatz = qnn_circuit\n",
    "\n",
    "# Combine the feature map with the ansatz\n",
    "full_circuit = QuantumCircuit(num_qubits)\n",
    "full_circuit.compose(?, range(num_qubits), inplace=True)\n",
    "full_circuit.compose(?, range(num_qubits), inplace=True)\n",
    "\n",
    "# Display the circuit\n",
    "?.decompose().?(\"mpl\", style=\"clifford\", fold=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have not increased the depth of the circuit. Let's see if we have increased its ability to model our images.\n",
    "\n",
    "### Step 2 revisited\n",
    "\n",
    "On a real device, we will need to transpile this new circuit for running on a real quantum backend.\n",
    "We use a simulator.\n",
    "Let's skip this step for now <span style=\"color: blue; font-weight: bold;\">to see if our revision of the variational circuit has had the desired effect on simulators.</span> \n",
    "\n",
    "### Step 3 revisited\n",
    "\n",
    "We now apply the updated model to our training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.primitives import StatevectorEstimator as Estimator\n",
    "\n",
    "batch_size = 140\n",
    "num_epochs = 1\n",
    "num_samples = len(train_images)\n",
    "\n",
    "# Globals\n",
    "circuit = full_circuit\n",
    "estimator = Estimator()  # simulator for debugging\n",
    "observables = observable\n",
    "objective_func_vals = []\n",
    "iter = 0\n",
    "\n",
    "# Random initial weights for the ansatz\n",
    "np.random.seed(42)\n",
    "weight_params = np.random.rand(len(ansatz.parameters)) * 2 * np.pi\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range((num_samples - 1) // batch_size + 1):\n",
    "        print(f\"Epoch: {epoch}, batch: {i}\")\n",
    "        start_i = i * batch_size\n",
    "        end_i = start_i + batch_size\n",
    "        train_images_batch = np.array(train_images[start_i:end_i])\n",
    "        train_labels_batch = np.array(train_labels[start_i:end_i])\n",
    "        input_params = train_images_batch\n",
    "        target = train_labels_batch\n",
    "        iter = 0\n",
    "        res = minimize(\n",
    "            mse_loss_weights, weight_params, method=\"COBYLA\", options={\"maxiter\": 100}\n",
    "        )\n",
    "        weight_params = res[\"x\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 revisited\n",
    "\n",
    "Let's start by checking whether our optimizer fully converged.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_func_vals_revised = objective_func_vals\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(obj_func_vals_revised, label=\"revised ansatz\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue; font-weight: bold;\">This does not appear fully converged</span>, as the loss function has not remained roughly level for substantially many steps. \n",
    "\n",
    "<span style=\"color: blue; font-weight: bold;\">\n",
    "But the loss function is already ~60% lower than when using the previous variational circuit. \n",
    "    \n",
    "\n",
    "If this were a research project, we would want to ensure full convergence. But for the purposes of exploration, this is sufficient. \n",
    "    \n",
    "\n",
    "Let's check the accuracy on our training and testing data.</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from qiskit.primitives import StatevectorEstimator as Estimator  # simulator\n",
    "# from qiskit_ibm_runtime import EstimatorV2 as Estimator  # real quantum computer\n",
    "\n",
    "estimator = Estimator()\n",
    "# estimator = Estimator(backend=backend)\n",
    "\n",
    "pred_train = forward(circuit, np.array(train_images), res[\"x\"], estimator, observable)\n",
    "# pred_train = forward(circuit_ibm, np.array(train_images), res['x'], estimator, observable_ibm)\n",
    "\n",
    "print(pred_train)\n",
    "\n",
    "pred_train_labels = copy.deepcopy(pred_train)\n",
    "pred_train_labels[pred_train_labels >= 0] = ?\n",
    "pred_train_labels[pred_train_labels < 0] = ?\n",
    "print(pred_train_labels)\n",
    "print(train_labels)\n",
    "\n",
    "accuracy = accuracy_score(?, ?)\n",
    "print(f\"Train accuracy: {? * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pred_train_labels))\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = forward(circuit, np.array(test_images), res[\"x\"], estimator, observable)\n",
    "# pred_test = forward(circuit_ibm, np.array(test_images), res['x'], estimator, observable_ibm)\n",
    "\n",
    "print(pred_test)\n",
    "\n",
    "pred_test_labels = copy.deepcopy(pred_test)\n",
    "pred_test_labels[pred_test_labels >= 0] = ?\n",
    "pred_test_labels[pred_test_labels < 0] = ?\n",
    "print(pred_test_labels)\n",
    "print(test_labels)\n",
    "\n",
    "accuracy = accuracy_score(?, ?)\n",
    "print(f\"Test accuracy: {? * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pred_test_labels))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue; font-weight: bold;\">\n",
    "\n",
    "$100\\%$ Accuracy on both sets! Our suspicion about accurate detection of horizontal lines being sufficient was correct!\n",
    "\n",
    "Further, our mapping from required information about the pixels to the CNOT gates in the quantum circuit was sufficiently effective.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\\[1] [https://arxiv.org/abs/2405.00781](https://arxiv.org/abs/2405.00781)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF NOTEBOOK"
   ]
  }
 ],
 "metadata": {
  "description": "Quantum variational circuits are applied to the recognition of patterns in an image. The parallels with classical neural networks are discussed.",
  "kernelspec": {
   "display_name": "Python 3 [Default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "title": "QVCs and QNNs"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
